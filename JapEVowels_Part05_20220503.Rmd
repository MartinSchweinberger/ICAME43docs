---
title: "A Corpus-Based Acoustic Analysis of Monophthong Vowels among Japanese Learners and Native Speakers of English - Part 2"
author: "Martin Schweinberger"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: word_document
---

# Introduction

This R Notebook shows how to access Praat from R and how to extract formant values from wav files and their corresponding Praat TextGrids. The tutorials is based on [Phonetics Tools in R](https://marissabarlaz.github.io/portfolio/phoneticstools/] by Marissa Barlaz).

## Preparation

install packages

```{r install, eval=F, message=F, warning=F}
# install
install.packages("tidyverse")
install.packages("here")
install.packages("adehabitatHR")
install.packages("lme4")
install.packages("sjPlot")
install.packages("report")
install.packages("flextable")
install.packages("cowplot")     
install.packages("randomForest") 
install.packages("rms") 
```

load packages

```{r load, message=F, warning=F}
library(tidyverse)
library(here)
library(adehabitatHR)
library(lme4)
library(sjPlot)
library(report)
library(flextable)
library(cowplot)      
library(randomForest) 
library(rms)    
# set options
options(stringsAsFactors = F)                           
options(scipen = 999) 
options(max.print=10000)
```


load data


```{r data, message=F, warning=F}
# load .rda data
fdat  <- base::readRDS(file = here::here("data", "fdat.rda")) 
# inspect
nrow(fdat); head(fdat)
```

# Reduce data

```{r redux, message=F, warning=F}
bdat <- fdat %>%
  dplyr::rename(YearsStay = Yrs.of.Stay....Yrs.,
                SpeakerType = ENS.Type,
                Occupation = Major..Occupation) %>%
  dplyr::select(-tmin, -tmax, -midpoint, -F1, -F2, -F3, -tgender, -tformants, -YearsStay, 
                -Test, - Score, -target_f1, -target_f2, -target_f3, -normF1, -normF2, -cF1, -cF2,
                -file, -Country, -edist, -barkF1, -barkF2, -Code, -id, -speaker,
                -vowel) %>%
  dplyr::mutate(label = stringr::str_remove_all(label, ":"))
# inspect
head(bdat)
```



# Split data

```{r}
nsd <- bdat %>%
  dplyr::filter(type == "ENS",
                tvariety == "us") %>%
  dplyr::select(-type, -tvariety) %>%
  dplyr::mutate_if(is.character, factor)
# inspect
head(nsd)
```




# MuPDARF


Now, we perform a random forest analysis of the native speaker data.


Now, we create a data set of only native speaker data.

```{r l2amp_03_11, echo=T, eval = T, message=FALSE, warning=FALSE}
nsd <- ampicle %>%
  dplyr::filter(Language == "English") %>%
  dplyr::select(-Learner, -Language)
head(nsd); str(nsd)
```

Now, we perform a random forest analysis of the native speaker data.

```{r l2amp_03_13, echo=T, eval = T, message=FALSE, warning=FALSE}
#           RANDOM FOREST: NATIVE-SPEAKRES
# set seed
set.seed(20200204)
nsrf <- randomForest(very ~ ., data=nsd, ntree=3000, proximity=TRUE,
                     #keep.forest=FALSE, 
                     importance=TRUE)
# inspect rf results
nsrf 
```

Next, we plot the results.

```{r l2amp_03_15, echo=T, eval = T, message=FALSE, warning=FALSE}
plot(nsrf)
```

Now, we plot the out-of-bag error frequencies.

```{r l2amp_03_21, echo=T, eval = T, message=FALSE, warning=FALSE}
# plot new precision/error rate
oob.error.data <- data.frame(
  Trees=rep(1:nrow(nsrf$err.rate), times=2),
  Type=rep(c("OOB", "very"), each=nrow(nsrf$err.rate)),
  Error=c(nsrf$err.rate[,"OOB"],
          nsrf$err.rate[,"1"]))
ggplot(data=oob.error.data, aes(x=Trees, y=Error)) +
  geom_line(aes(color=Type))
```

Now, we check the error rates and accuracy and also check how much the model performs better than a base-line model.

```{r l2amp_03_23, echo=T, eval = T, message=FALSE, warning=FALSE}
# determine accuracy by prediction
# install package
#source("https://bioconductor.org/biocLite.R"); biocLite(); library(Biobase)
#install.packages("Biobase", repos=c("http://rstudio.org/_packages", "http://cran.rstudio.com", 
#                                      "http://cran.rstudio.com/", dependencies=TRUE))
#install.packages("dimRed", dependencies = TRUE)
#install.packages('caret', dependencies = TRUE)
# load caret library
library(caret) # because initially caret did not work, the libraries above had to confusionMatrix(ptrain, train$very)
pnsrf <- predict(nsrf, nsd)
confusionMatrix(pnsrf, nsd$very)
# calculate increase in prediction accuracy compared to base-line model
0.8462/0.5066
```

Now, we inspect which variables are important for the predictions.

```{r l2amp_03_33, echo=T, eval = T, message=FALSE, warning=FALSE}
png("images/VarImpRF1.png",  width = 700, height = 480) # save plot
varImpPlot(nsrf, main = "", pch = 20, cex = 1.5) 
dev.off()
varImpPlot(nsrf, main = "", pch = 20, cex = 1.5)
```

```{r l2amp_03_39, echo=T, eval = T, message=FALSE, warning=FALSE}
library(Hmisc)
ampred <- as.numeric(predict(nsrf, nsd))
ampred <- ifelse(ampred == 1, 0, 1)
test <- as.numeric(nsd$very) 
test <- ifelse(test == 1, 0, 1)
somers2(ampred, test) 
```


Now, we use the random forest analysis of the native speakers to predict how a native speaker would have amplified the adjectives in the non-native speaker data. In a first step, we extract only non-native speaker data.

```{r l2amp_03_43, echo=T, eval = T, message=FALSE, warning=FALSE}
#           RANDOM FOREST: NON-NATIVE-SPEAKERS
nnsd <- ampicle %>%
  dplyr::filter(Language != "English") %>%
  droplevels()
head(nnsd); str(nnsd)
```

Next, we use the random forest analysis of the native speakers to predict how a native speaker would have amplified the adjectives.

```{r l2amp_03_45, echo=T, eval = T, message=FALSE, warning=FALSE}
# extract prediction for training data
pnns <- predict(nsrf, nnsd) 
# inspect predictions
head(pnns); head(nnsd$very)  
```

Now, we create a confusion matrix to check the accuracy of the prediction

```{r l2amp_03_49, echo=T, eval = T, message=FALSE, warning=FALSE}
confusionMatrix(pnns, nnsd$very)
# calculate increase in prediction accuracy compared to base-line model
0.6163/0.5324
```

The prediction accuracy increases by 13.35 percent if use use our model compared to a no information model.

```{r l2amp_03_51, echo=T, eval = T, message=FALSE, warning=FALSE}
verynnsd <- nnsd
verynnsd$NSvery <- predict(nsrf, nnsd)
verynnsd <- verynnsd %>%
  dplyr::rename(NNSvery = very) %>%
  dplyr::select(Language, Adjective, NNSvery, NSvery) %>%
  dplyr::group_by(Language, Adjective) %>%
  dplyr::summarise(NAdj = n(),
                   FrqNNSVery = sum(as.numeric(as.character(NNSvery))),
                   FrqNSVery = sum(as.numeric(as.character(NSvery)))) %>%
  dplyr::mutate(a = NAdj-FrqNNSVery,
                b = FrqNNSVery,
                c = NAdj-FrqNSVery,
                d = FrqNSVery) %>%
  dplyr::rowwise() %>%
  dplyr::mutate(p = as.vector(unlist(fisher.test(matrix(c(a, b, c, d), ncol = 2, byrow = T), simulate.p.value=TRUE)[1]))) %>%
    dplyr::mutate(x2 = as.vector(unlist(chisq.test(matrix(c(a, b, c, d), ncol = 2, byrow = T), simulate.p.value=TRUE)[1]))) %>%
  dplyr::mutate(phi = sqrt((x2/(a + b + c + d)))) %>%
  dplyr::mutate(RateVeryNS = round(FrqNSVery/NAdj*100, 2),
                RateVeryNNS = round(FrqNNSVery/NAdj*100, 2)) %>%
  dplyr::mutate(Type = ifelse(FrqNSVery > FrqNNSVery, "Underuse",
                              ifelse(FrqNNSVery > FrqNSVery, "Overuse", "Equal"))) %>%
    dplyr::mutate(Significance = ifelse(p <= .05, "p<.001",
                ifelse(p <= .01, "p<.01",
                       ifelse(p <= .001, "p<.001", "n.s.")))) %>%
  dplyr::mutate(NRows = nrow(verynnsd)) %>%
  dplyr::ungroup() %>%
  dplyr::arrange(p) %>%
  dplyr::mutate(j = 1:n()) %>%
  # perform benjamini-holm correction
  dplyr::mutate(corr05 = ((j/NRows)*0.05)) %>%
  dplyr::mutate(corr01 = ((j/NRows)*0.01)) %>%
  dplyr::mutate(corr001 = ((j/NRows)*0.001)) %>%
  # calculate corrected significance status
  dplyr::mutate(CorrSignificance = ifelse(p <= corr001, "p<.001",
                ifelse(p <= corr01, "p<.01",
                       ifelse(p <= corr001, "p<.001", "n.s.")))) %>%
  dplyr::mutate(p = round(p, 6)) %>%
  dplyr::mutate(x2 = round(x2, 1)) %>%
  dplyr::mutate(phi = round(phi, 1)) %>%
  dplyr::select(-a, -b, -c, -d, - j, -NRows, -corr05, -corr01, -corr001,
                -Significance)
head(verynnsd)
```

We now reduce the table and select only those rows which contain Bonferroni corrected significant results.

```{r l2amp_03_53, echo=T, eval = T, message=FALSE, warning=FALSE}
verynnsd <- verynnsd %>%
  dplyr::filter(CorrSignificance != "n.s.") %>%
  dplyr::filter(Adjective != "other")
# save results
write.table(verynnsd, "datatables/verynnsd_sigdiff.txt", sep = "\t", row.names = F)
# inspect results
verynnsd
```

```{r l2amp_03_55, echo=T, eval = T, message=FALSE, warning=FALSE}
library(Hmisc)
ampred <- as.numeric(predict(nsrf, nnsd))
ampred <- ifelse(ampred == 1, 0, 1)
test <- as.numeric(nnsd$very) 
test <- ifelse(test == 1, 0, 1)
somers2(ampred, test) 
```

Next, we add the difference between predictions and observed amplification to the data.

```{r l2amp_03_57, echo=T, eval = T, message=FALSE, warning=FALSE}
# add native choice prediction to data
nnsd$NativeChoice <- as.vector(pnns)
nnsd$NativeChoice <- as.factor(nnsd$NativeChoice)
# code if choice of nns is nativelike or not
nnsd$very <- as.character(nnsd$very)
nnsd$NativeChoice <- as.character(nnsd$NativeChoice)
nnsd$NonNativeLike <- ifelse(nnsd$very == nnsd$NativeChoice, 0, 1)
nnsd$very <- NULL
# inspect new data
head(nnsd)
```

Now, we perform a regression analysis on then difference between native speakers and non-native speakers. We begin by creating fixed-effects intercept-only base-line models.

```{r l2amp_03_59, echo=T, eval = T, message=FALSE, warning=FALSE}
# load library
library(rms)
# set options
options(contrasts  =c("contr.treatment", "contr.poly"))
nnsd.dist <- datadist(nnsd)
options(datadist = "nnsd.dist")
# generate initial minimal regression model 
# baseline model glm
m0.glm = glm(NonNativeLike ~ 1, family = binomial, data = nnsd) 
# baseline model lrm
#m0.lrm = rms::lrm(NonNativeLike ~ 1, data = nnsd, x = T, y = T) 
# inspect results
summary(m0.glm)
```

Now, we create a lmer mixed-effects intercept-only base-line models.

```{r l2amp_03_61, echo=T, eval = T, message=FALSE, warning=FALSE}
# load library
library(lme4)
library(car)
# create model with random intercepts for file and token
m0.glmer <- glmer(NonNativeLike ~ (1|Adjective) + (1|Learner), data = nnsd, family = binomial)
# results of the lmer object
print(m0.glmer, corr = F)
```

[@baayen2008analyzing 278-284] uses the function above but this function is no longer up-to-date because the "family" parameter is deprecated we switch to glmer (suggested by R) and also create a lmer object of the final minimal adequate model as some functions do not (yet) work on glmer.

We continue by creating glmer objects with different random-effect structures.

```{r l2amp_03_65, echo=T, eval = T, message=FALSE, warning=FALSE}
m0.glmer = glmer(NonNativeLike ~ (1|Adjective) + (1|Learner), data = nnsd, family = binomial, control=glmerControl(optimizer="bobyqa"))
# check if including the random effect is permitted by comparing the aic from the glm to aic from the glmer model
aic.m0.glmer <- AIC(logLik(m0.glmer))
aic.glm <- AIC(logLik(m0.glm))
aic.m0.glmer; aic.glm
```

The AIC of the glmer object is smaller than the AIC of the glm object which means that including random intercepts for Adjectives is justified. 

We continue by renaming the base-line model and test if including the random effect structure significantly improves model fit.

```{r l2amp_03_67, echo=T, eval = T, message=FALSE, warning=FALSE}
m0.glmer = glmer(NonNativeLike ~ (1|Adjective) + (1|Learner), data = nnsd, family = binomial, control=glmerControl(optimizer="bobyqa"))
# test random effects
null.id = -2 * logLik(m0.glm) + 2 * logLik(m0.glmer)
pchisq(as.numeric(null.id), df=1, lower.tail=F) # sig m0.glmer better than m0.glm
```

We can now begin with the model fitting to find the "best" model, i.e. the minimal adequate model with is the model that explains the maximum amount of variance with a minimum number of predictors. The cut-off for collinearity is 3 for main effects and 30 for interactions. To arrive at a final minimal adequate model, we use a step-wise step up procedure.

In a first step, we add Language.

```{r l2amp_03_69, echo=T, eval = T, message=FALSE, warning=FALSE}
# add Language
ifelse(min(ftable(nnsd$Language, nnsd$NonNativeLike)) == 0, "not possible", "possible")
m1.glm <- update(m0.glm, .~.+Language)
m1.glmer <- update(m0.glmer, .~.+Language)
anova(m1.glmer, m0.glmer, test = "Chi") 
Anova(m1.glmer, type = "III", test = "Chi")
```

As Language is  significant. We therefore retain it in the model and we continue by adding RelativeFrequency.

```{r l2amp_03_71, echo=T, eval = T, message=FALSE, warning=FALSE}
m2.glm <- update(m1.glm, .~.+RelativeFrequency)
vif(m2.glm)
m2.glmer <- update(m1.glmer, .~.+RelativeFrequency)
anova(m2.glmer, m1.glmer, test = "Chi")
Anova(m2.glmer, type = "III", test = "Chi")
```

As RelativeFrequency is significant, we retain it in the model and we continue by adding Function.

```{r l2amp_03_73, echo=T, eval = T, message=FALSE, warning=FALSE}
ifelse(min(ftable(nnsd$Function, nnsd$NonNativeLike)) == 0, "not possible", "possible")
m3.glm <- update(m2.glm, .~.+ Function)
vif(m3.glm)
m3.glmer <- update(m2.glmer, .~.+ Function)
anova(m3.glmer, m2.glmer, test = "Chi") 
Anova(m3.glmer, type = "III", test = "Chi")
```

As Function is not significant, we do not retain it in the model and we continue by adding Gradability.

```{r l2amp_03_75, echo=T, eval = T, message=FALSE, warning=FALSE}
m4.glm <- update(m2.glm, .~.+Gradability)
vif(m4.glm)
m4.glmer <- update(m2.glmer, .~.+Gradability)
anova(m4.glmer, m2.glmer, test = "Chi")
Anova(m4.glmer, type = "III", test = "Chi")
```

As Gradability is not significant, we do not retain it in the model and we continue by adding SemanticCategory.

```{r l2amp_03_77, echo=T, eval = T, message=FALSE, warning=FALSE}
# add SemanticCategory
ifelse(min(ftable(nnsd$SemanticCategory, nnsd$NonNativeLike)) == 0, "not possible", "possible")
m5.glm <- update(m2.glm, .~.+ SemanticCategory)
vif(m5.glm)
m5.glmer <- update(m2.glmer, .~.+ SemanticCategory)
anova(m5.glmer, m2.glmer, test = "Chi")
Anova(m5.glmer, type = "III", test = "Chi")
```

As SemanticCategory is significant, we retain it in the model and we continue by adding Emotionality.

```{r l2amp_03_79, echo=T, eval = T, message=FALSE, warning=FALSE}
# add Emotionality
ifelse(min(ftable(nnsd$Emotionality, nnsd$NonNativeLike)) == 0, "not possible", "possible")
m6.glm <- update(m5.glm, .~.+Emotionality)
vif(m6.glm)
m6.glmer <- update(m5.glmer, .~.+Emotionality)
anova(m6.glmer, m5.glmer, test = "Chi") 
Anova(m6.glmer, type = "III", test = "Chi")
```

As Emotionality is significant, we retain it in the model and we continue by adding Priming.

```{r l2amp_03_81, echo=T, eval = T, message=FALSE, warning=FALSE}
# add Priming
ifelse(min(ftable(nnsd$Priming, nnsd$NonNativeLike)) == 0, "not possible", "possible")
m7.glm <- update(m6.glm, .~.+Priming)
vif(m7.glm)
m7.glmer <- update(m6.glmer, .~.+Priming)
anova(m7.glmer, m6.glmer, test = "Chi") 
Anova(m7.glmer, type = "III", test = "Chi")
```

As Priming is significant, we retain it in the model and we continue by adding all two-way interactions. In a first step, we check, we two-way interactions we need to include.

```{r l2amp_03_83, echo=T, eval = T, message=FALSE, warning=FALSE}
# find all 2-way interactions
library(utils)
#colnames(nnsd)
# define variables included in interactions
vars <- c("Language", "Function", "RelativeFrequency", "Gradability",
          "SemanticCategory", "Emotionality", "Priming")
intac <- t(combn(vars, 2))
intac
```

We now start testing two-way interactions and we begin by adding the interaction between Language and syntactic Function.

```{r l2amp_03_85, echo=T, eval = T, message=FALSE, warning=FALSE}
# add Language * Function
ifelse(min(ftable(nnsd$Language, nnsd$Function, nnsd$NonNativeLike)) == 0, "not possible", "possible")
m8.glm <- update(m7.glm, .~.+ Language * Function) 
vif(m8.glm)
```

As including the interaction between Language and syntactic Function leads to an unacceptable level of collinearity (vifs exceeding values of 30), we do not retain it in the model and we continue by adding the interaction between Language and  the relative frequency of adjectives.

```{r l2amp_03_87, echo=T, eval = T, message=FALSE, warning=FALSE}
# add Language * RelativeFrequency
m9.glm <- update(m7.glm, .~.+ Language * RelativeFrequency)      
vif(m9.glm)
```

As including the interaction between Language and RelativeFrequency leads to an unacceptable level of collinearity (vifs exceeding values of 30), we do not retain it in the model and we continue by adding the interaction between Language and Gradability.

```{r l2amp_03_89, echo=T, eval = T, message=FALSE, warning=FALSE}
# add Language * Gradability      
m10.glm <- update(m7.glm, .~.+ Language * Gradability)
vif(m10.glm)
m10.glmer <- update(m7.glmer, .~.+Language * Gradability)
anova(m10.glmer, m7.glmer, test = "Chi") 
Anova(m10.glmer, type = "III", test = "Chi")
```

As including the interaction between Language and Gradability leads to an unacceptable level of collinearity (vifs exceeding values of 30)and because it is not significant, we do not retain it in the model and we continue by adding the interaction between Language and SemanticCategory.

```{r l2amp_03_91, echo=T, eval = T, message=FALSE, warning=FALSE}
# add Language * SemanticCategory
ifelse(min(ftable(nnsd$Language, nnsd$SemanticCategory, nnsd$NonNativeLike)) == 0, "not possible", "possible")
m11.glm <- update(m7.glm, .~.+ Language * SemanticCategory)
vif(m11.glm)
```

As including the interaction between Language and SemanticCategory leads to an unacceptable level of collinearity (vifs exceeding values of 30) and we continue by adding the interaction between Language and Emotionality.

```{r l2amp_03_93, echo=T, eval = T, message=FALSE, warning=FALSE}
# add Language * Emotionality 
ifelse(min(ftable(nnsd$Language, nnsd$Emotionality, nnsd$NonNativeLike)) == 0, "not possible", "possible")      
m12.glm <- update(m7.glm, .~.+ Language * Emotionality)
vif(m12.glm)
```

As including the interaction between Language and Emotionality leads to an unacceptable level of collinearity (vifs exceeding values of 30), we do not retain it in the model and we continue by adding the interaction between Language and Priming.

```{r l2amp_03_95, echo=T, eval = T, message=FALSE, warning=FALSE}
# add Language * Priming 
ifelse(min(ftable(nnsd$Language, nnsd$Priming, nnsd$NonNativeLike)) == 0, "not possible", "possible")      
m13.glm <- update(m7.glm, .~.+ Language * Priming)
vif(m13.glm)
m13.glmer <- update(m7.glmer, .~.+Language * Priming)
anova(m13.glmer, m7.glmer, test = "Chi") 
Anova(m13.glmer, type = "III", test = "Chi")
```

As including the interaction between Language and Priming is not significant, we do not retain it in the model and we continue by adding the interaction between Function and RelativeFrequency.

```{r l2amp_03_99, echo=T, eval = T, message=FALSE, warning=FALSE}
# add Function * RelativeFrequency
m14.glm <- update(m7.glm, .~.+ Function * RelativeFrequency)
vif(m14.glm)
m14.glmer <- update(m7.glmer, .~.+ Function * RelativeFrequency)
anova(m14.glmer, m7.glmer, test = "Chi")
Anova(m14.glmer, type = "III", test = "Chi")
```

As including the interaction between syntactic Function and RelativeFrequency  is significant, we retain it in the model and continue by adding the interaction between Function and Gradability.

```{r l2amp_03_101, echo=T, eval = T, message=FALSE, warning=FALSE}
# add Function * Gradability
m15.glm <- update(m14.glm, .~.+ Function * Gradability)
vif(m15.glm)
m15.glmer <- update(m14.glmer, .~.+ Function * Gradability)
anova(m15.glmer, m14.glmer, test = "Chi")
Anova(m15.glmer, type = "III", test = "Chi")
```

As including the interaction between syntactic Function and Gradability  is not significant, we do not retain it in the model and continue by adding the interaction between Function and SemanticCategory.

```{r l2amp_03_103, echo=T, eval = T, message=FALSE, warning=FALSE}
# add Function * SemanticCategory
ifelse(min(ftable(nnsd$Function, nnsd$SemanticCategory, nnsd$NonNativeLike)) == 0, "not possible", "possible")
m16.glm <- update(m15.glm, .~.+ Function * SemanticCategory)
vif(m16.glm) 
```

As including the interaction between syntactic Function and SemanticCategory leads to an unacceptable level of collinearity (vifs exceeding values of 30), we do not retain it in the model and we continue by adding the interaction between Function and Emotionality.

```{r l2amp_03_105, echo=T, eval = T, message=FALSE, warning=FALSE}
# add Function * Emotionality      
ifelse(min(ftable(nnsd$Function, nnsd$Emotionality, nnsd$NonNativeLike)) == 0, "not possible", "possible")
m17.glm <- update(m15.glm, .~.+ Function * Emotionality)
vif(m17.glm)
m17.glmer <- update(m15.glmer, .~.+ Function * Emotionality)
anova(m17.glmer, m15.glmer, test = "Chi")
Anova(m17.glmer, type = "III", test = "Chi")
```

As including the interaction between syntactic Function and Emotionality is  significant, we retain it in the model and continue by adding the interaction between syntactic Function and Priming.

```{r l2amp_03_107, echo=T, eval = T, message=FALSE, warning=FALSE}
m18.glm <- update(m17.glm, .~.+ Function * Priming)
vif(m18.glm)
m18.glmer <- update(m17.glmer, .~.+ Function * Priming)
anova(m18.glmer, m17.glmer, test = "Chi") 
Anova(m18.glmer, type = "III", test = "Chi")
```

As including the interaction between syntactic Function and Priming is significant, we retain it in the model and continue by adding the interaction between RelativeFrequency and Gradability.

```{r l2amp_03_111, echo=T, eval = T, message=FALSE, warning=FALSE}
# add RelativeFrequency * Gradability
m19.glm <- update(m18.glm, .~.+ RelativeFrequency * Gradability)
vif(m19.glm)
m19.glmer <- update(m18.glmer, .~.+ RelativeFrequency * Gradability)
anova(m19.glmer, m18.glmer, test = "Chi") 
Anova(m19.glmer, type = "III", test = "Chi")
```

As including the interaction between RelativeFrequency and Gradability is not significant, we do not retain it in the model and continue by adding the interaction between RelativeFrequency and SemanticCategory.

```{r l2amp_03_113, echo=T, eval = T, message=FALSE, warning=FALSE}
# add RelativeFrequency * SemanticCategory
m20.glm <- update(m18.glm, .~.+ RelativeFrequency * SemanticCategory)
vif(m20.glm)
```

As including the interaction between RelativeFrequency and SemanticCategory leads to an unacceptable level of collinearity (vifs exceeding values of 30), we do not retain it in the model and we continue by adding the interaction between RelativeFrequency and Emotionality.

```{r l2amp_03_115, echo=T, eval = T, message=FALSE, warning=FALSE}
# add RelativeFrequency * Emotionality
m21.glm <- update(m18.glm, .~.+ RelativeFrequency * Emotionality)
vif(m21.glm)
m21.glmer <- update(m18.glmer, .~.+ RelativeFrequency * Emotionality)
anova(m21.glmer, m18.glmer, test = "Chi") 
```

As including the interaction between RelativeFrequency and SemanticCategory  is not  significant, we do not retain it in the model and we continue by adding the interaction between RelativeFrequency and Priming.

```{r l2amp_03_117, echo=T, eval = T, message=FALSE, warning=FALSE}
m22.glm <- update(m18.glm, .~.+ RelativeFrequency * Priming)
vif(m22.glm)
m22.glmer <- update(m18.glmer, .~.+ RelativeFrequency * Priming)
anova(m22.glmer, m18.glmer, test = "Chi") 
Anova(m22.glmer, type = "III", test = "Chi")
```

As including the interaction between RelativeFrequency and Priming is not  significant, we do not retain it in the model and continue by adding the interaction between Gradability and SemanticCategory.

```{r l2amp_03_119, echo=T, eval = T, message=FALSE, warning=FALSE}
# add Gradability * SemanticCategory
m23.glm <- update(m18.glm, .~.+ Gradability * SemanticCategory)
vif(m23.glm)
m23.glmer <- update(m18.glmer, .~.+ Gradability * SemanticCategory)
anova(m23.glmer, m18.glmer, test = "Chi") 
```

As including the interaction between Gradability and SemanticCategory leads to an unacceptable level of collinearity (vifs exceeding values of 30) and because it is not significant, we do not retain it in the model and we continue by adding the interaction between Gradability and Emotionality.

```{r l2amp_03_121, echo=T, eval = T, message=FALSE, warning=FALSE}
# add Gradability * Emotionality
m24.glm <- update(m18.glm, .~.+ Gradability * Emotionality)
vif(m24.glm)
m24.glmer <- update(m18.glmer, .~.+ Gradability * Emotionality)
anova(m24.glmer, m18.glmer, test = "Chi") 
Anova(m24.glmer, type = "III", test = "Chi")
```

As including the interaction between Gradability and Emotionality is not significant, we do not retain it in the model we do not include it in the model  and continue by adding the interaction between SemanticCategory and Priming. 

```{r l2amp_03_125, echo=T, eval = T, message=FALSE, warning=FALSE}
# add Gradability * Priming
m25.glm <- update(m24.glm, .~.+ Gradability * Priming)
vif(m25.glm)
m25.glmer <- update(m24.glmer, .~.+ Gradability * Priming)
anova(m25.glmer, m24.glmer, test = "Chi") 
```

As including the interaction between Gradability and Priming is not significant, we do not retain it in the model we do not include it in the model  and continue by adding the interaction between SemanticCategory and Emotionality. 

```{r l2amp_03_127, echo=T, eval = T, message=FALSE, warning=FALSE}
# add SemanticCategory * Emotionality
ifelse(min(ftable(nnsd$SemanticCategory, nnsd$Emotionality, nnsd$NonNativeLike)) == 0, "not possible", "possible")
```

As including the interaction between SemanticCategory and Emotionality would lead to incomplete information (variable levels are not filled),  we continue by adding the interaction between SemanticCategory and Priming.


```{r l2amp_03_129, echo=T, eval = T, message=FALSE, warning=FALSE}
# add SemanticCategory * Priming
m26.glm <- update(m24.glm, .~.+ SemanticCategory * Priming)
vif(m26.glm)
m26.glmer <- update(m24.glmer, .~.+ Emotionality * Priming)
anova(m26.glmer, m24.glmer, test = "Chi") 
```

As including the interaction between SemanticCategory and Priming is not significant,  we continue by adding the interaction between SemanticCategory and Priming.

```{r l2amp_03_130, echo=T, eval = T, message=FALSE, warning=FALSE}
# add Emotionality * Priming
m27.glm <- update(m24.glm, .~.+ Emotionality * Priming)
vif(m27.glm)
m27.glmer <- update(m24.glmer, .~.+ Emotionality * Priming)
anova(m27.glmer, m24.glmer, test = "Chi") 
```

As including the interaction between Emotionality and Priming is not significant, we do not include it in the model. In a next step, we check, we three-way interactions we need to include.

```{r l2amp_03_131, echo=T, eval = T, message=FALSE, warning=FALSE}
# find all 2-way interactions
library(utils)
#colnames(nnsd)
# define variables included in interactions
vars <- c("Language", "Function", "RelativeFrequency", "Gradability",
          "SemanticCategory", "Emotionality", "Priming")
intac <- t(combn(vars, 3))
intac
```

Next, we create the glm models.

```{r l2amp_03_133, echo=T, eval = T, message=FALSE, warning=FALSE}
m28.glm <- update(m24.glm, .~.+  Language*Function*RelativeFrequency)
m29.glm <- update(m24.glm, .~.+  Language*Function*Gradability)
m30.glm <- update(m24.glm, .~.+  Language*Function*SemanticCategory)
m31.glm <- update(m24.glm, .~.+  Language*Function*Emotionality)
m32.glm <- update(m24.glm, .~.+  Language*Function*Priming)
m33.glm <- update(m24.glm, .~.+  Language*RelativeFrequency*Gradability)
m34.glm <- update(m24.glm, .~.+  Language*RelativeFrequency*SemanticCategory)
m35.glm <- update(m24.glm, .~.+  Language*RelativeFrequency*Emotionality)
m36.glm <- update(m24.glm, .~.+  Language*RelativeFrequency*Priming)
m37.glm <- update(m24.glm, .~.+  Language*Gradability*SemanticCategory)
m38.glm <- update(m24.glm, .~.+  Language*Gradability*Emotionality)
m39.glm <- update(m24.glm, .~.+  Language*Gradability*Priming)
m40.glm <- update(m24.glm, .~.+  Language*SemanticCategory*Emotionality)
m41.glm <- update(m24.glm, .~.+  Language*SemanticCategory*Priming)
m42.glm <- update(m24.glm, .~.+  Language*Emotionality*Priming)
m43.glm <- update(m24.glm, .~.+  Function*RelativeFrequency*Gradability)
m44.glm <- update(m24.glm, .~.+  Function*RelativeFrequency*SemanticCategory)
m45.glm <- update(m24.glm, .~.+  Function*RelativeFrequency*Emotionality)
m46.glm <- update(m24.glm, .~.+  Function*RelativeFrequency*Priming)
m47.glm <- update(m24.glm, .~.+  Function*Gradability*SemanticCategory)
m48.glm <- update(m24.glm, .~.+  Function*Gradability*Emotionality)
m49.glm <- update(m48.glm, .~.+  Function*Gradability*Priming)
m50.glm <- update(m48.glm, .~.+  Function*SemanticCategory*Emotionality)
m51.glm <- update(m48.glm, .~.+  Function*SemanticCategory*Priming)
m52.glm <- update(m48.glm, .~.+  Function*Emotionality*Priming)
m53.glm <- update(m48.glm, .~.+  RelativeFrequency*Gradability*SemanticCategory)
m54.glm <- update(m48.glm, .~.+  RelativeFrequency*Gradability*Emotionality)
m55.glm <- update(m48.glm, .~.+  RelativeFrequency*Gradability*Priming)
m56.glm <- update(m48.glm, .~.+  RelativeFrequency*SemanticCategory*Emotionality)
m57.glm <- update(m48.glm, .~.+  RelativeFrequency*SemanticCategory*Priming)
m58.glm <- update(m48.glm, .~.+  RelativeFrequency*Emotionality*Priming)
m59.glm <- update(m48.glm, .~.+  Gradability*SemanticCategory*Emotionality)
m60.glm <- update(m48.glm, .~.+  Gradability*SemanticCategory*Priming)
m61.glm <- update(m48.glm, .~.+  Gradability*Emotionality*Priming)
m62.glm <- update(m48.glm, .~.+  SemanticCategory*Emotionality*Priming)
# extract vifs
ifelse(max(vif(m28.glm))> 100, "HighVIFs", "VIFsOK")
#ifelse(max(vif(m29.glm))> 100, "HighVIFs", "VIFsOK") # aliased coefficients 
#ifelse(max(vif(m30.glm))> 100, "HighVIFs", "VIFsOK") # aliased coefficients 
#ifelse(max(vif(m31.glm))> 100, "HighVIFs", "VIFsOK") # aliased coefficients 
#ifelse(max(vif(m32.glm))> 100, "HighVIFs", "VIFsOK") # aliased coefficients
ifelse(max(vif(m33.glm))> 100, "HighVIFs", "VIFsOK")
ifelse(max(vif(m34.glm))> 100, "HighVIFs", "VIFsOK")
ifelse(max(vif(m35.glm))> 100, "HighVIFs", "VIFsOK")
ifelse(max(vif(m36.glm))> 100, "HighVIFs", "VIFsOK")
ifelse(max(vif(m37.glm))> 100, "HighVIFs", "VIFsOK")
ifelse(max(vif(m38.glm))> 100, "HighVIFs", "VIFsOK")
ifelse(max(vif(m39.glm))> 100, "HighVIFs", "VIFsOK")  
#ifelse(max(vif(m40.glm))> 100, "HighVIFs", "VIFsOK") # aliased coefficients 
#ifelse(max(vif(m41.glm))> 100, "HighVIFs", "VIFsOK") # aliased coefficients 
#ifelse(max(vif(m42.glm))> 100, "HighVIFs", "VIFsOK") # aliased coefficients
ifelse(max(vif(m43.glm))> 100, "HighVIFs", "VIFsOK") # ok
ifelse(max(vif(m44.glm))> 100, "HighVIFs", "VIFsOK")
ifelse(max(vif(m45.glm))> 100, "HighVIFs", "VIFsOK") 
ifelse(max(vif(m46.glm))> 100, "HighVIFs", "VIFsOK")# ok
ifelse(max(vif(m47.glm))> 100, "HighVIFs", "VIFsOK") 
ifelse(max(vif(m48.glm))> 100, "HighVIFs", "VIFsOK") # ok
ifelse(max(vif(m49.glm))> 100, "HighVIFs", "VIFsOK") # ok
#ifelse(max(vif(m50.glm))> 100, "HighVIFs", "VIFsOK")# aliased coefficients 
ifelse(max(vif(m51.glm))> 100, "HighVIFs", "VIFsOK")
ifelse(max(vif(m52.glm))> 100, "HighVIFs", "VIFsOK")
ifelse(max(vif(m53.glm))> 100, "HighVIFs", "VIFsOK") 
ifelse(max(vif(m54.glm))> 100, "HighVIFs", "VIFsOK") # ok
ifelse(max(vif(m55.glm))> 100, "HighVIFs", "VIFsOK") # ok
#ifelse(max(vif(m56.glm))> 100, "HighVIFs", "VIFsOK") # aliased coefficients
ifelse(max(vif(m57.glm))> 100, "HighVIFs", "VIFsOK") 
ifelse(max(vif(m58.glm))> 100, "HighVIFs", "VIFsOK") # ok
#ifelse(max(vif(m59.glm))> 100, "HighVIFs", "VIFsOK") # aliased coefficients 
ifelse(max(vif(m60.glm))> 100, "HighVIFs", "VIFsOK") 
ifelse(max(vif(m61.glm))> 100, "HighVIFs", "VIFsOK") # ok
#ifelse(max(vif(m62.glm))> 100, "HighVIFs", "VIFsOK") # aliased coefficients 
```

Next, we create the glmer models where the VIFs were acceptable.

```{r l2amp_03_135, echo=T, eval = T, message=FALSE, warning=FALSE}
m43.glmer <- update(m24.glmer, .~.+  Function*RelativeFrequency*Gradability)
m46.glmer <- update(m24.glmer, .~.+  Function*RelativeFrequency*Priming)
m48.glmer <- update(m24.glmer, .~.+  Function*Gradability*Emotionality)
m49.glmer <- update(m48.glmer, .~.+  Function*Gradability*Priming)
m54.glmer <- update(m48.glmer, .~.+  RelativeFrequency*Gradability*Emotionality)
m55.glmer <- update(m48.glmer, .~.+  RelativeFrequency*Gradability*Priming)
m58.glmer <- update(m48.glmer, .~.+  RelativeFrequency*Emotionality*Priming)
m61.glmer <- update(m48.glmer, .~.+  Gradability*Emotionality*Priming)
```

Now, we compare the models to see if we need to retain predictors.

```{r l2amp_03_137, echo=T, eval = T, message=FALSE, warning=FALSE}
anova(m43.glmer, m24.glmer, test = "Chi")[8] 
anova(m46.glmer, m24.glmer, test = "Chi")[8]  
anova(m48.glmer, m24.glmer, test = "Chi")[8]  
anova(m49.glmer, m48.glmer, test = "Chi")[8]  
anova(m54.glmer, m48.glmer, test = "Chi")[8]  
anova(m55.glmer, m48.glmer, test = "Chi")[8]  
anova(m58.glmer, m48.glmer, test = "Chi")[8]  
anova(m61.glmer, m48.glmer, test = "Chi")[8]  
```

```{r l2amp_03_139, echo=T, eval = T, message=FALSE, warning=FALSE}
Anova(m48.glmer, type = "III", test = "Chi")
```
have arrived at our  final minimal adequate model.

We will now summarize the results of this final model.

```{r l2amp_03_141, echo=T, eval = T, message=FALSE, warning=FALSE}
# load function for regression table summary
source("D:\\R/meblr.summary.tworandom.R")
# set up summary table
meblrm_nnsd <- meblrm.summary(m0.glm, m48.glm, m0.glmer, m48.glmer, nnsd$NonNativeLike) #
# save results to disc
write.table(meblrm_nnsd, "datatables/meblrm_nnsd.txt", sep="\t")
# show results
meblrm_nnsd
```

```{r l2amp_03_143, echo=T, eval = T, message=FALSE, warning=FALSE}
# load function
library(car)
meblrm_nnsd_Anova <- Anova(m48.glmer, type = "III", test = "Chi")
# save results to disc
write.table(meblrm_nnsd_Anova, "datatables/meblrm_nnsd_Anova.txt", sep="\t")
# show results
meblrm_nnsd_Anova
```


```{r l2amp_03_149, echo=T, eval = T, message=FALSE, warning=FALSE}
# use customized model comparison function
# create comparisons
m1.m0 <- anova(m1.glmer, m0.glmer, test = "Chi")
m2.m1 <- anova(m2.glmer, m1.glmer, test = "Chi")
m3.m2 <- anova(m3.glmer, m2.glmer, test = "Chi")
m4.m2 <- anova(m4.glmer, m2.glmer, test = "Chi")
m5.m2 <- anova(m5.glmer, m2.glmer, test = "Chi")
m6.m5 <- anova(m6.glmer, m5.glmer, test = "Chi")
m7.m6 <- anova(m7.glmer, m6.glmer, test = "Chi")
m10.m7 <- anova(m10.glmer, m7.glmer, test = "Chi")
m13.m7 <- anova(m13.glmer, m7.glmer, test = "Chi")
m14.m7 <- anova(m14.glmer, m7.glmer, test = "Chi")
m15.m14 <- anova(m15.glmer, m14.glmer, test = "Chi")
m17.m15 <- anova(m17.glmer, m15.glmer, test = "Chi")
m18.m17 <- anova(m18.glmer, m17.glmer, test = "Chi")
m19.m18 <- anova(m19.glmer, m18.glmer, test = "Chi")
m21.m18 <- anova(m21.glmer, m18.glmer, test = "Chi")
m22.m18 <- anova(m22.glmer, m18.glmer, test = "Chi")
m23.m18 <- anova(m23.glmer, m18.glmer, test = "Chi")
m24.m18 <- anova(m24.glmer, m18.glmer, test = "Chi")
m25.m24 <- anova(m25.glmer, m24.glmer, test = "Chi")
m26.m24 <- anova(m26.glmer, m24.glmer, test = "Chi")
m27.m24 <- anova(m27.glmer, m24.glmer, test = "Chi")
m43.m24 <- anova(m43.glmer, m24.glmer, test = "Chi")
m46.m24 <- anova(m46.glmer, m24.glmer, test = "Chi")
m48.m24 <- anova(m48.glmer, m24.glmer, test = "Chi")
m49.m48 <- anova(m49.glmer, m48.glmer, test = "Chi")
m54.m48 <- anova(m54.glmer, m48.glmer, test = "Chi")
m55.m48 <- anova(m55.glmer, m48.glmer, test = "Chi")
m58.m48 <- anova(m58.glmer, m48.glmer, test = "Chi")
m61.m48 <- anova(m61.glmer, m48.glmer, test = "Chi")
# create a list of the model compariNonNativeLikens
mdlcmp <- list(m1.m0, m2.m1, m3.m2, m4.m2, m5.m2, m6.m5, m7.m6, 
               m10.m7, m13.m7, m14.m7, m15.m14, m17.m15, m18.m17, 
               m19.m18, m21.m18, m22.m18, m23.m18, m24.m18, m25.m24, 
               m26.m24, m27.m24, m43.m24, m46.m24, m48.m24, m49.m48, 
               m54.m48, m55.m48, m58.m48, m61.m48)
# load function
#source("D:\\R/ModelFittingSummarySWSU.R") # for Mixed Effects Model fitting (step-wise step-up): Binary Logistic Mixed Effects Models
# apply function
#mdl.cmp.glmersc.swsu.dm <- mdl.fttng.swsu(mdlcmp)
# save results
#write.table(mdl.cmp.glmersc.swsu.dm, "datatables/mdl_cmp_glmersc_swsu_nnsd.txt", sep="\t")
# inspect output
#mdl.cmp.glmersc.swsu.dm
```

Create table for plotting

```{r l2amp_03_151, echo=T, eval = T, message=FALSE, warning=FALSE}
library(effects)
png("images/effectsfinalmodel.png",  width = 960, height = 480) 
plot(allEffects(m48.glmer), type="response", ylim=c(0,1), grid=TRUE, 
     lines = list(col="black",
                  lty = 1,
                  confint=list(style="bars",
                               col = "grey80")), 
     ylab = "Probability (non-target-like amplification)")
dev.off()
plot(allEffects(m48.glmer), type="response", ylim=c(0,1), grid=TRUE, 
     lines = list(col="black",
                  lty = 1,
                  confint=list(style="bars",
                               col = "grey80")), 
     ylab = "Probability (non-target-like amplification)")
```


```{r l2amp_03_153, echo=T, eval = T, message=FALSE, warning=FALSE}
# predict probs of nativelike for effects
nnsd$Prediction <- predict(m48.glmer, nnsd, type="response")
# rename data set
pd <- nnsd
# inspect predictions
summary(pd$Prediction)
```

Start visualizing the effects.

```{r l2amp_03_155, echo=T, eval = T, message=FALSE, warning=FALSE}
# start plotting
p1 <- ggplot(pd, aes(Language, Prediction)) +
  #  facet_wrap(vars(Language)) +
    geom_point(aes(reorder(Language, Prediction, function(Prediction) -mean(Prediction)), y=Prediction), size = NA) +
  stat_summary(fun.y = mean, geom = "point", size = .5) +
  stat_summary(fun.y = mean, geom = "line") +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", 
               width = 0.2, size = 1) +
  coord_cartesian(ylim = c(0, 1)) +
  theme_set(theme_light(base_size = 12)) +
  theme(legend.position="none", 
        axis.text.x = element_text(size=10, angle=90)) +
  scale_color_manual(values = c("grey30"))
p1 <- ggplot(pd, aes(Language, Prediction)) +
  #  facet_wrap(vars(Language)) +
    geom_point(aes(reorder(Language, Prediction, function(Prediction) -mean(Prediction)), y=Prediction), size = NA) +
  stat_summary(fun.y = mean, geom = "point", size = .5) +
  stat_summary(fun.y = mean, geom = "line") +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2, size = 1) +
  coord_cartesian(ylim = c(0, 1)) +
  theme_set(theme_light(base_size = 12)) +
  theme(legend.position="none", 
 #       panel.grid.major = element_blank(), 
 #       panel.grid.minor = element_blank(), 
        axis.text.x = element_text(size=10, angle=90)) +
  labs(x = "Language", y = "Predicted probability of \nnon-target-like choices") +
  scale_color_manual(values = c("grey30")) +
  ggsave(file = paste(imageDirectory,"PredProbNNLLanguage.png",sep="/"), 
       height = 3,  width = 5, dpi = 320)
# activate (remove #) to show
p1
```

```{r l2amp_03_157, echo=T, eval = T, message=FALSE, warning=FALSE}
# start plotting
p2 <- ggplot(pd, aes(SemanticCategory, Prediction)) +
  #  facet_wrap(vars(Language)) +
    geom_point(aes(reorder(SemanticCategory, Prediction, function(Prediction) -mean(Prediction)), y=Prediction), size = NA) +
  stat_summary(fun.y = mean, geom = "point", size = .5) +
  stat_summary(fun.y = mean, geom = "line") +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2, size = 1) +
  coord_cartesian(ylim = c(0, 1)) +
  theme_set(theme_light(base_size = 10)) +
  theme(legend.position="none", 
 #       panel.grid.major = element_blank(), 
 #       panel.grid.minor = element_blank(), 
        axis.text.x = element_text(size=10, angle=90)) +
  labs(x = "Semantic Category", y = "Predicted probability of \nnon-target-like choices") +
  scale_color_manual(values = c("grey30")) +
  ggsave(file = paste(imageDirectory,"PredProbNNLSemanticCategory.png",sep="/"), 
       height = 4,  width = 5, dpi = 320)
# activate (remove #) to show
p2
```

```{r l2amp_03_159, echo=T, eval = T, message=FALSE, warning=FALSE}
# start plot: all with zero
p3 <- ggplot(pd, aes(x = RelativeFrequency, y = Prediction, color = Function)) +
  geom_smooth(method = "lm", se = F) +
    scale_colour_manual(values=c("gray30", "gray80"),
                      name=c(""),
                      breaks=c("Attributive", "Predicative"),  
                      labels = c("Attributive", "Predicative")) +
  theme_set(theme_light(base_size = 15)) +
  theme(legend.position="top", 
        axis.text.x = element_text(size=15),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  coord_cartesian(ylim = c(0, 1)) +
  labs(x = "Scaled relative frequency (adj. type)", y = "Predicted probability of \nnon-target-like choices") +
  guides(size = FALSE)+
  guides(alpha = FALSE) +
  ggsave(file = paste(imageDirectory,"PredictionNNSFrequencyFunction.png",
                      sep="/"), height = 3,  width = 5, dpi = 320)
p3
```

```{r l2amp_03_161, echo=T, eval = T, message=FALSE, warning=FALSE}
# start plotting
p4 <- ggplot(pd, aes(Priming, Prediction, color = Function)) +
#    geom_point(aes(Emotionality, Prediction), size = NA) +
  stat_summary(fun.y = mean, geom = "point") +
  stat_summary(fun.y = mean, geom = "line") +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2) +
  scale_colour_manual(values=c("gray30", "gray80"),
                      name=c(""),
                      breaks=c("Attributive", "Predicative"),  
                      labels = c("Attributive", "Predicative")) +
  coord_cartesian(ylim = c(0, 1)) +  
  labs(x = "Priming", 
       y = "Predicted probability of \nnon-target-like choices") +
  theme_set(theme_light(base_size = 15)) +
  theme(legend.position = "top",
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        axis.text.x = element_text(size=10, angle=90)) +
  guides(size = FALSE) +
  guides(alpha = FALSE) +
  ggsave(file = paste(imageDirectory,"PredProbNNLPrimingFunction.png",
                      sep="/"), height = 5,  width = 5, dpi = 320)
p4
```

```{r l2amp_03_163, echo=T, eval = T, message=FALSE, warning=FALSE}
# relevel 
pd$Emotionality <- factor(pd$Emotionality, 
                          levels = c("NegativeEmotional", 
                                     "NonEmotional", "PositiveEmotional"))
# start plotting
p5 <- ggplot(pd, aes(Gradability, Prediction, color = Function)) +
  facet_grid( ~ Emotionality) +
  geom_smooth(method = "lm", se = F) +
  scale_colour_manual(values=c("gray30", "gray80"),
                      name=c(""),
                      breaks=c("Attributive", "Predicative"),  
                      labels = c("Attributive", "Predicative")) +
  coord_cartesian(ylim = c(0, 1)) +  
  labs(x = "Gradability", 
       y = "Predicted probability of \nnon-target-like choices") +
  theme_set(theme_bw(base_size = 15)) +
  theme(legend.position = "top",
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        axis.text.x = element_text(size=10, angle=0)) +
  guides(size = FALSE) +
  guides(alpha = FALSE) +
  ggsave(file = paste(imageDirectory,"PredProbNNLEmotionalityFunctionGradability.png",
                      sep="/"), height = 5,  width = 8, dpi = 320)
p5
```

```{r l2amp_03_171, echo=T, eval = T, message=FALSE, warning=FALSE}
randomtb <- ranef(m48.glmer)$Adjective
rndmlngtb <- data.frame(rownames(randomtb), randomtb)
colnames(rndmlngtb) <- c("Adjective", "Intercept")
rndmlngtb <- rndmlngtb[order(rndmlngtb$Intercept, decreasing = T),]
summary(rndmlngtb$Intercept)
```

```{r l2amp_03_172, echo=T, eval = T, message=FALSE, warning=FALSE}
p6 <- ggplot(rndmlngtb, aes(Adjective, Intercept)) +
  geom_point(aes(reorder(Adjective, -Intercept, fun = Intercept), y=Intercept)) +
  coord_cartesian(ylim = c(-2, 3.5)) +
  theme_set(theme_bw(base_size = 15)) +
  theme(legend.position="none",
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        axis.text.x = element_text(size=10, angle=90)) +
  labs(x = "Adjective type", y = "Adjustment to Intercept") +
  ggsave(file = paste(imageDirectory,"RanAdjective.png",sep="/"), 
       height = 5,  width = 7,  dpi = 320)
# activate (remove #) to show
p6
```

```{r l2amp_03_173, echo=T, eval = T, message=FALSE, warning=FALSE}
summary(ranef(m48.glmer)$Learner)
```

```{r l2amp_03_175, echo=T, eval = T, message=FALSE, warning=FALSE}
randomtb <- ranef(m48.glmer)$Learner
rndmlngtb <- data.frame(rownames(randomtb), randomtb)
colnames(rndmlngtb) <- c("Learner", "Intercept")
rndmlngtb <- rndmlngtb[order(rndmlngtb$Intercept, decreasing = T),]

p7 <- ggplot(rndmlngtb, aes(Learner, Intercept)) +
  geom_point(aes(reorder(Learner, -Intercept, fun = Intercept), y=Intercept)) +
  coord_cartesian(ylim = c(-1.5, 1.5)) +
  theme_set(theme_bw(base_size = 15)) +
  theme(legend.position="none",
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        axis.text.x = element_text(size=1, angle=90)) +
  labs(x = "Learner", y = "Adjustment to Intercept") +
  ggsave(file = paste(imageDirectory,"RanLearner.png",sep="/"), 
       height = 5,  width = 7,  dpi = 320)
# activate (remove #) to show
p7
```

```{r l2amp_03_177, echo=T, eval = T, message=FALSE, warning=FALSE}
# summary tables
tb1 <- ampicle %>%
  dplyr::select(Language, very) %>%
  dplyr::mutate(very = as.numeric(ifelse(very == "0", 0, 1))) %>%
  dplyr::group_by(Language) %>%
  dplyr::summarise(Nvery = sum(very),
                Nadj = n(),
                Pvery = round(Nvery/Nadj*100, 1))
tb2 <- ampicle %>%
  dplyr::select(Language, Learner) %>%
  unique() %>%
  dplyr::group_by(Language) %>%
  dplyr::summarise(NSpeakers = n())
Table1 <- left_join(tb1, tb2)
# save table
write.table(Table1, "datatables/Table0.txt", sep = "\t", row.names = F)
Table1
```

```{r l2amp_03_179, echo=T, eval = T, message=FALSE, warning=FALSE}
# summary tables
tb2 <- ampicle %>%
  dplyr::mutate(very = as.numeric(ifelse(very == "0", 0, 1)))
tb2a <- tb2 %>%
  dplyr::summarise(Learners = length(names(table(Learner))),
                Nvery = sum(very),
                Nadj = n(),
                Pvery = round(Nvery/Nadj*100, 1))
tb2a <- cbind("Total", tb2a)
tb2b <- tb2 %>%
  dplyr::group_by(Language) %>%
  dplyr::summarise(Learners = 
                     length(names(table(Learner)[which(table(Learner) != 0)])),
                   Nvery = sum(very),
                   Nadj = n(),
                   Pvery = round(Nvery/Nadj*100, 1))
tb2c <- tb2 %>%
  dplyr::group_by(Emotionality) %>%
  dplyr::summarise(Learners = 
                     length(names(table(Learner)[which(table(Learner) != 0)])),
                   Nvery = sum(very),
                   Nadj = n(),
                   Pvery = round(Nvery/Nadj*100, 1))
tb2d <- tb2 %>%
  dplyr::group_by(Priming) %>%
  dplyr::summarise(Learners = 
                     length(names(table(Learner)[which(table(Learner) != 0)])),
                   Nvery = sum(very),
                   Nadj = n(),
                   Pvery = round(Nvery/Nadj*100, 1))
tb2e <- tb2 %>%
  dplyr::group_by(SemanticCategory) %>%
  dplyr::summarise(Learners = 
                     length(names(table(Learner)[which(table(Learner) != 0)])),
                   Nvery = sum(very),
                   Nadj = n(),
                   Pvery = round(Nvery/Nadj*100, 1))
colnames(tb2a)[1] <- "Levels"
colnames(tb2b)[1] <- "Levels"
colnames(tb2c)[1] <- "Levels"
colnames(tb2d)[1] <- "Levels"
colnames(tb2e)[1] <- "Levels"
Table2p <- rbind(tb2b, tb2c, tb2d, tb2e, tb2a)
Variables <- c(rep("Language", nrow(tb2b)),
               rep("Emotionality", nrow(tb2c)),
               rep("Priming", nrow(tb2d)),
               rep("SemanticCategory", nrow(tb2e)),
               "Total")
Table2 <- cbind(Variables, Table2p)
# save table
write.table(Table2, "datatables/Table2.txt", sep = "\t", row.names = F)
Table2
```

```{r l2amp_03_179, echo=T, eval = T, message=FALSE, warning=FALSE}
library(MuMIn)
r.squaredGLMM(m48.glmer)
```

We have reached the end of part 3 of the analysis.


# Citation & Session Info

Schweinberger, Martin. `r format(Sys.time(), '%Y')`. A Corpus-Based Acoustic Analysis of Monophthongal Vowels among Japanese Learners and Native Speakers of English. Brisbane: The University of Queensland, School of Languages and Cultures. url: https://slcladal.github.io/praatrf.html (Version `r format(Sys.time(), '%Y.%m.%d')`).

@manual{schweinberger`r format(Sys.time(), '%Y')`praatrf,
  author = {Schweinberger, Martin},
  title = {A Corpus-Based Acoustic Analysis of Monophthongal Vowels among Japanese Learners and Native Speakers of English},
  note = {https://slcladal.github.io/praatrf.html},
  year = {`r format(Sys.time(), '%Y')`},
  organization = "The University of Queensland, School of Languages and Cultures},
  address = {Brisbane},
  edition = {`r format(Sys.time(), '%Y.%m.%d')`}
}


```{r}
sessionInfo()
```
